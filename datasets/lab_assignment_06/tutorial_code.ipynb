{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a tutorial to create a simple prediction model to perform the following\n",
    "# 1. Read in and show basic information about the training data\n",
    "# 2. Create a simple prediction model on a portion of the training data\n",
    "# 3. Test the quality of the model on a later portion of the data\n",
    "# 4. Create a final model using all the training data based on the best choices above\n",
    "# 5. Apply that model to the test data, to be scored on the kaggle.com site\n",
    "\n",
    "# FILES NEEDED: for this code to work, you will need train_luc.csv and test_luc.csv\n",
    "# in the same folder as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data and display the first 5 rows\n",
    "train_df = pd.read_csv('train_luc.csv', header=0)\n",
    "\n",
    "print(\"\\nNumber of samples:\",train_df.shape[0],\"and number of features:\",train_df.shape[1],\"\\n\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding basic stats of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read about the data elsewhere, however, it is critical to observe the data to make sure\n",
    "# everything is read in correctly and matches the description\n",
    "\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's take datetime (which isn't very useful to algorithms) and turn it into something useful.\n",
    "# e.g. this will create a new column for the hour\n",
    "def hour_of_day(dt):\n",
    "    return datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S\").time().hour\n",
    "train_df['hour'] = train_df['datetime'].map(hour_of_day)\n",
    "train_df.head()\n",
    "# note the new column on the right labelled \"hour\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make visualizations to better understand your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's take a look at the amerage amount of bike use for each hour of the day\n",
    "# as a \"sanity check\" to make sure the data makes sense before going further\n",
    "hours = np.unique(train_df['hour'])\n",
    "print(\"hours:\",hours)\n",
    "\n",
    "hours_mean = {}\n",
    "for h in hours:\n",
    "    temp_df = train_df.loc[train_df['hour'] == h]\n",
    "    hours_mean[h] = temp_df['count'].mean()\n",
    "\n",
    "# plot the results. Maybe should see peaks from bike commuting or evening use\n",
    "plt.bar(hours,[hours_mean[h] for h in hours])\n",
    "plt.xlabel(\"hour\")\n",
    "plt.ylabel(\"average number of bikes used\")\n",
    "plt.title(\"Measured bike use over 2 years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick the features and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick your features\n",
    "cols = ['hour'] # clearly a simple model\n",
    "# try more features later, like...\n",
    "# cols = ['hour','season']\n",
    "\n",
    "# pick your model (you should consider adjusting optional parameters too)\n",
    "# reading in a few models we can pick from (there are many others)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# pick one by commenting/uncommenting\n",
    "model = DecisionTreeRegressor()\n",
    "#model = LinearRegression()\n",
    "#model = KNeighborsRegressor(n_neighbors = 5)\n",
    "#model = svm.LinearSVR()\n",
    "#model = RandomForestRegressor(n_estimators = 1)\n",
    "\n",
    "print(\"columns selected for later:\",cols)\n",
    "print(model) # to get an idea of parameters and confirm model chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate your training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a way of splitting training and testing by hand\n",
    "# however, there are tools to do this automatically\n",
    "# google \"cross validation\" for a better/more advanced strategy\n",
    "#--------------------\n",
    "\n",
    "n = len(train_df) # get number of rows in the training set\n",
    "training_size = 0.75 # fraction of training data to split off for internal testing\n",
    "\n",
    "# set up separate training and testing sets\n",
    "# in this case using shuffled array indices\n",
    "# there are many more ways to do this too\n",
    "indices = np.array(range(n)) # makes an array of row indices in order\n",
    "from numpy.random import shuffle\n",
    "shuffle(indices)\n",
    "split_point = int(n*training_size)\n",
    "mytrain_i = indices[0:split_point]\n",
    "mytest_i = indices[split_point:]\n",
    "\n",
    "# now use those shuffled indices to separating training from test dataframes\n",
    "new_train_df = train_df.iloc[mytrain_i]\n",
    "new_test_df = train_df.iloc[mytest_i]\n",
    "\n",
    "print(\"samples in the new training subset:\",len(new_train_df))\n",
    "print(\"samples in the new test subset:\",len(new_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model to a portion of the training set, test on the rest and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model to the training subset of original training data\n",
    "model.fit(new_train_df[cols], new_train_df['count'])\n",
    "\n",
    "# predict on the testing subset of the original training data\n",
    "pred_count = model.predict(new_test_df[cols])\n",
    "\n",
    "# score the model on the new test set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rms = np.sqrt(mean_squared_error(new_test_df['count'],pred_count))\n",
    "print(\"RMS error:\",rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the test file output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the test data\n",
    "test_df = pd.read_csv('test_luc.csv', header=0)\n",
    "print(\"\\nNumber of samples:\",test_df.shape[0] ,\"and number of features:\",test_df.shape[1],\"\\n\")\n",
    "\n",
    "# must add that new feature into the test data too, to use it in prediction\n",
    "test_df['hour'] = test_df['datetime'].map(hour_of_day)\n",
    "\n",
    "# show the test data output to be sure it read in correctly and added the column\n",
    "test_df.head()\n",
    "\n",
    "# fit the selected model TO YOUR FULL TRAINING SET\n",
    "model.fit( train_df[cols], train_df['count'])\n",
    "\n",
    "# apply to the test data FOR WHICH YOU DON'T HAVE THE ANSWERS\n",
    "# (not the \"test set\" you used for model selection and tuning)\n",
    "pred_count = model.predict(test_df[cols])\n",
    "\n",
    "# add the prediction column (in case you want to inspect it later)\n",
    "test_df['count'] = pred_count\n",
    "\n",
    "# save the predicted count as a csv with a header column and datetime row\n",
    "test_df = test_df[['datetime','count']].to_csv('my_prediction.csv', \n",
    "    index=False, header=True)\n",
    "print(\"Prediction complete. Saved as my_prediction.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
